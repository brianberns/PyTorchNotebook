{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can do machine learning\n",
    "\n",
    "These are some ideas that helped me understand machine learning in PyTorch.\n",
    "\n",
    "## Step 1. Developing in Python\n",
    "\n",
    "### Python\n",
    "\n",
    "Python is an interpreted language that is very popular in data science. It's slow but very flexible. Dynamic typing makes it perfect for torturing actual software developers.\n",
    "\n",
    "Python has \"virtual environments\" instead of projects. The package manager is called `pip`.\n",
    "\n",
    "### Visual Studio Code\n",
    "\n",
    "VS Code seems to be a popular, cross-platform app for Python development. It's a text editor, not a full-fledged IDE, but there is a rich marketplace of extensions that provide language-specific behavior.\n",
    "\n",
    "### Jupyter notebooks\n",
    "\n",
    "Notebooks are a cool way to meld markdown with code, creating executable documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(-6, 7)\n",
    "y = [ 1 / (1 + math.exp(-z)) for z in x ]   # sigmoid function\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ranges in Python are [inclusive, exclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. PyTorch\n",
    "\n",
    "PyTorch is an open source machine learning framework created by Meta that is becoming more popular than Google's Tensorflow. It can run on either the CPU (slow) or GPU (fast, called \"CUDA\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Tensors\n",
    "\n",
    "A tensor is an N-dimensional array.\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*6Z892ClZGon03_Mawj4Pew.png\" width=\"400\"/>\n",
    "\n",
    "### 0-dimensional tensor\n",
    "\n",
    "Also known as a scalar. Here we create a tensor from a single numeric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim0 = torch.tensor(3.1415)\n",
    "print(ndim0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-dimensional tensor\n",
    "\n",
    "Also known as a vector. One way to create a 1D tensor is from a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim1 = torch.tensor([1, 2, 4, 8, 16, 32])\n",
    "print(ndim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A vector can also be interpreted as a position in N-dimensional space. For example, the vector above is a point in 6D space. Both interpretations are valid, but don't get them mixed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-dimensional tensor\n",
    "\n",
    "Also known as a matrix, which is essentially a table of rows and columns. Here, we generate a 1D tensor of 12 integers, and then change its shape to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim1 = torch.arange(0, 12)\n",
    "ndim2 = ndim1.view(3, 4)   # 3 rows by 4 columns\n",
    "print(ndim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly modify the data in the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim2[1,0] = -100\n",
    "print(ndim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the view shares its data with the original tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transpose a tensor's rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndim2.transpose(0, 1))   # transpose dim-0 and dim-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a 3D tensor with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim3 = torch.rand(2, 3, 4)   # 2 layers X 3 rows X 4 columns\n",
    "print(ndim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask any tensor for its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ndim3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimensions\n",
    "\n",
    "It gets harder to visualize tensors as the number of dimensions increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim4 = torch.rand(2, 3, 2, 3)   # 2 hyperlayers X 3 layers X 2 rows X 3 columns\n",
    "print(ndim4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dimensionality\n",
    "\n",
    "Start with a 1D vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.arange(-4, 5)   # 1D: 9 columns\n",
    "print(vector)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we insert a dimension of size 1 in front, the columns stay columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsq0 = vector.unsqueeze(0)   # 2D: 1 row x 9 columns\n",
    "print(unsq0)                  # note the extra pair of brackets in the output\n",
    "print(unsq0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we insert a dimension of size 1 at the end, the columns become rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsq1 = vector.unsqueeze(1)   # 2D: 9 rows x 1 column\n",
    "print(unsq1)\n",
    "print(unsq1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Neural network building blocks\n",
    "\n",
    "Some examples from the \"nn\" zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, these are functions of type `Tensor -> Tensor`, so a complex neural network can be built by composition. During training, the model learns the best values for the parameters inside these blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "Applies a linear transformation: **y = xW<sup>T</sup> + B**\n",
    "\n",
    "Where:\n",
    "* **x** is the input tensor\n",
    "* **y** is the output tensor\n",
    "* **W** is a 2D tensor of weight parameters\n",
    "* **B** is a 1D tensor of bias parameters\n",
    "* **xW<sup>T</sup>** is matrix multiplication of **x** by the transpose of **W**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(in_features = 20, out_features = 30)\n",
    "print(\"Weight:\", linear.weight.shape)\n",
    "print(\"Bias:\", linear.bias.shape)\n",
    "print(\"Total # of parms:\", sum(parm.numel() for parm in linear.parameters()))   # (30 x 20) + 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear transforms are useful for \"projecting\" a tensor into a different shape with the same number of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 6, 20)   # last dimension must match linear input\n",
    "print(\"Input:\", x.shape)\n",
    "y = linear(x)              # result is still 3D, but now with 30 columns\n",
    "print(\"Output:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "#### ReLU\n",
    "\n",
    "Activation functions provide non-linear transformations. They typically don't have any parameters. One simple activation function is \"rectified linear unit\" (ReLU), which maps any negative input to zero, and any non-negative input to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "x = torch.arange(-10, 11)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sets some of the input to zero during training (and the remaining elements are scaled up proportionally). Dropout prevents the model from becoming too reliant on a small set of parameters.\n",
    "\n",
    "A dropout layer has no parameters, but its dropout rate (e.g. 20%) is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.2)\n",
    "\n",
    "x = torch.ones(3, 4)\n",
    "print(x)\n",
    "y = dropout(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout is recomputed during each application, so results are not deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dropout(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "\n",
    "Normalizes input around its mean and standard deviation. This reduces training time by reining in large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numColumns = 4\n",
    "norm = nn.LayerNorm(numColumns)   # expect last dimension of this size\n",
    "\n",
    "x = torch.arange(0.0, 12.0).view(-1, numColumns)   # -1: PyTorch infers the # of rows\n",
    "print(x)\n",
    "y = norm(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization can have weight and bias parameters that the model learns during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight:\", norm.weight)\n",
    "print(\"Bias:\", norm.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "An embedding is a lookup table that maps scalar indexes to vectors, where each vector represents the embedding of the corresponding key into a higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings = 4, embedding_dim = 3)\n",
    "key = torch.tensor(0)\n",
    "v = embedding(key)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding vectors are learnable parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Building a model\n",
    "\n",
    "### Non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have simple non-linear function that we want a neural network to learn. (This is like using a bomb to kill an ant, but it illustrates the basic principle well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetFunc(x):\n",
    "    return 3 * x ** 2 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some training data, consisting of inputs to and outputs from the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 6.0\n",
    "xBatch = torch.arange(-domain, domain+1).unsqueeze(1)   # each row is a distinct input\n",
    "print(\"Input:\\n\", xBatch)\n",
    "\n",
    "yTarget = targetFunc(xBatch)                            # each output row corresponds to a specific input\n",
    "print(\"Output:\\n\", yTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've used tensors to create a dataset of inputs/outputs all at once, but you can also think of `xBatch` as a single input, with corresponding output `yTarget`.\n",
    "\n",
    "If we stick to thinking of these as pairs of input/output scalars, the target function is a parabola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xBatch, yTarget)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to approximate a parabola with a pair of linear functions sandwiched around ReLU, none of which are curved?\n",
    "\n",
    "Since the target function is `scalar -> scalar`, the model should have a single input value and a single output value, but we can give the model lots of parameters in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 10),   # scalar -> 10-dimensional space\n",
    "    nn.ReLU(),          # non-linear\n",
    "    nn.Linear(10, 1))   # 10-dimensional space -> scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's parameters are initialized with random values by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, parm) in model.named_parameters():\n",
    "    print(\"\\n{}: {}\".format(name, parm.data.shape))\n",
    "    print(\"   \", parm.data)\n",
    "\n",
    "numParms = sum(parm.numel() for parm in model.parameters())\n",
    "print(\"\\nTotal # of parameters:\", numParms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need:\n",
    "* A loss function that determines how far off target the model is, and\n",
    "* An optimizer that will attempt to minimize the loss. The optimizer's \"learning rate\" determines how much of an adjustment it will make to the model's parameters on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunc = nn.MSELoss()   # mean squared error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)   # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to train the model. During each iteration:\n",
    "\n",
    "1. Make a forward pass to generate a batch of predicted values.\n",
    "2. Calculate the difference between the predicted and target values. This is called the \"loss\".\n",
    "3. Make a backward pass from the loss through the model to calculate gradients, which are the directions in which each parameter much be adjusted.\n",
    "4. Adjust each parameter by a small amount accordingly. We don't want to overshoot the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100000):\n",
    "\n",
    "    # forward pass\n",
    "    yPrediction = model(xBatch)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = lossFunc(yPrediction, yTarget)\n",
    "    if (epoch & (epoch-1) == 0):   # check for power of 2\n",
    "        print(\"Epoch {}, loss {}\".format(epoch, loss))\n",
    "        with torch.no_grad():\n",
    "            plt.plot(xBatch, yTarget)\n",
    "            plt.plot(xBatch, yPrediction)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # adjust parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned parameter values that approximate the target function well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, parm) in model.named_parameters() :\n",
    "    print(\"{}: {}\".format(name, parm.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
